# building centralperkGPT: a transformer from scratch

i built **centralperkGPT**, a lightweight gpt-style transformer implemented entirely from scratch in pytorch. the goal was not to train a production-ready model to compete with openai. instead, it was an exercise in deeply understanding the internal mechanics of language models—specifically observing how logits, attention maps, and training dynamics evolve over discrete iterations.

source code: [github.com/srikarboddupally/centralperkGPT](https://github.com/srikarboddupally/centralperkGPT)

---

## the motivation: breaking the black box

i've always leaned towards a "build to learn" methodology. reading whitepapers on transformer architectures is one thing, but you don't truly understand the black box until you are forced to write the matrix multiplications yourself. 

when you use high-level abstractions like hugging face's `Trainer` api, the magic is hidden. you pass in a dataset, and out comes a coherent text generator. i wanted to strip all of that away. no trainers, no pre-packaged pipelines—just pure pytorch assembled block by block. 

the architecture and training loop for this project were heavily inspired by andrej karpathy's brilliant "let's build gpt: from scratch" lecture. building the nano/min-gpt architecture from the ground up provides an unparalleled understanding of modern llm mechanics, and his walkthrough gave me the exact blueprint i needed to start coding.

## architecture & the mechanics of generation

to make centralperkGPT work, i had to implement several core components of the modern decoder-only architecture. exposing these pieces reveals exactly how a neural network learns to predict the next token:

* **causal self-attention:** this is the heart of the model. i implemented multi-head attention with a causal mask (a lower triangular matrix). this mask is crucial—it ensures that when the model is processing the $t$-th token, it can only attend to tokens at positions $1$ through $t$, preventing it from "cheating" by looking into the future.
* **positional and token embeddings:** transformers are inherently colorblind to the order of a sequence. without positional embeddings, the model wouldn't know if a word is at the beginning or the end of a sentence.
* **residual connections & layernorm:** as the network gets deeper, gradients tend to vanish. adding residual pathways allows the gradients to flow smoothly back to the early layers during backpropagation, stabilizing the entire training process.
* **custom training loop:** bypassing standard apis meant writing the forward pass, calculating the cross-entropy loss against the target tokens, and manually stepping the optimizer. 

## observing the training dynamics

one of the most fascinating parts of this project was watching the model learn in real-time. training on a small, constrained dataset (a collection of television scripts) yielded several observable phenomena that usually happen invisibly in massive clusters:

**the transition from noise to structure**
in the first few hundred iterations, the model outputs pure, uniform gibberish. the loss is high, and the logits are spread thinly across the vocabulary. but suddenly, the model figures out basic formatting. it learns that a character's name is usually capitalized and followed by a colon. it learns spacing. 

**attention self-organization**
as training progresses, the attention maps visibly self-organize. you can see specific attention heads taking on distinct roles—one head might focus heavily on the previous character, while another learns to track the current speaker across a long context window.

**the reality of overfitting**
because i trained this on a highly constrained dataset, it was a perfect laboratory for observing memorization. watching the model rapidly overfit and begin spitting out nearly exact quotes from the training data was a great practical lesson in the balance between parameter count, dataset size, and generalization.

## output sample

i trained the model autoregressively on character-level generation. while character-level models inevitably produce some spelling artifacts and glitches compared to subword tokenizers, the structural and conversational patterns clearly reflect the learned dialogue. 

here is an unedited snippet generated after training:

```text
SCENE: MONICA'S APARTMENT

RACHEL: My phone froze.
ROSS: Have you tried turning it off?
JOEY: Check the cluster.
CHANDLER: It says "Good luck".
```

## scaling up: next steps

building a functional toy model inevitably makes you realize what is required to make a fast, production-grade model. to scale this from an educational exercise to a more performant architecture, these are the optimizations i plan to explore next:

* **kv caching:** right now, autoregressive generation is slow because the model recalculates the attention for the entire sequence at every step. implementing a key-value cache would drastically reduce inference latency.
* **subword tokenization:** migrating from a character-level tokenizer to byte-pair encoding (bpe) would allow the model to process information much more efficiently.
* **weight-tying:** sharing weights between the embedding layer and the final linear output head to save parameters.
* **hardware-aware algorithms:** integrating flash attention to optimize memory reads/writes on the gpu.

if you want a deep, principled foundation in transformers, i highly recommend opening a blank python file and trying to build one yourself. 

credit again to andrej karpathy for the foundational blueprints.