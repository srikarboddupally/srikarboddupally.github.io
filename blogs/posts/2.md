---
title: "centralperkGPT â€” Building a GPT from Scratch"
description: "My journey learning how transformers really work by building centralperkGPT."
date: 2026-02-24
---

# centralperkGPT â€” Building a GPT from Scratch

I built **centralperkGPT**, a lightweight GPT-style transformer implemented entirely from scratch in PyTorch. The goal wasnâ€™t to compete with production models, but to go deep into *how language models actually work* â€” observing how logits, attention, and training dynamics evolve over iterations.

Check out the repo here:  
ğŸ‘‰ https://github.com/srikarboddupally/centralperkGPT

---

## ğŸ§  Why I Built It

I wanted to *demystify transformers* by writing one from first principles â€” no hugging-face trainers, no high-level abstractions â€” just pure PyTorch code assembled block by block.

My inspiration came from watching Andrej Karpathyâ€™s incredible video tutorial **â€œLetâ€™s build GPT: from scratch, in code, spelled outâ€** on YouTube. In that video, Karpathy walks through every piece of a GPT-style model, from tokenization to self-attention and training loops. If you want to *really understand how these models work from the ground up*, seriously go watch it:  
ğŸ‘‰ https://youtu.be/kCc8FmEb1nY?si=AFmQtJIPUkgTkIra :contentReference[oaicite:0]{index=0}

That video and the accompanying *nanoGPT/minGPT* family of repos have helped me more than I can say when it came to understanding the inner workings of modern LLMs. :contentReference[oaicite:1]{index=1}

---

## ğŸ› ï¸ What centralperkGPT Does

This project implements:

- A **decoder-only Transformer** architecture  
- **Causal self-attention** with multi-head heads  
- Positional + token embeddings  
- Residual connections + LayerNorm  
- Autoregressive character-level generation  
- A *custom training loop* (no Trainer API)

Unlike many tutorials that hide complexity behind frameworks, this code exposes core pieces so you can see exactly what every component does and how it contributes to learning. :contentReference[oaicite:2]{index=2}

---

## ğŸ“ˆ What I Observed

During training on a small custom dataset:

- The model **rapidly learns structure** in text  
- Attention patterns *organize meaningful contexts*  
- Overfitting happens quickly with small data â€” a great way to see memorization dynamics  
- Logits transition from noisy to highly confident over training iterations

Watching training dynamics â€” especially how logits sharpen and how attention maps evolve â€” was the moment when things stopped being magical and became *mathematically beautiful*.

---

## ğŸ§© Example Output

Hereâ€™s a snippet of sample output produced by centralperkGPT after training:
SCENE: MONICA'S APARTMENT

RACHEL: My phone froze.
ROSS: Have you tried turning it off?
JOEY: Check the cluster.
CHANDLER: It says "Good luck".


(Character-level sampling results in occasional glitches, but the structure and conversational style clearly reflect learned dialogue patterns.) :contentReference[oaicite:3]{index=3}

---

## ğŸ“š What Youâ€™ll Learn From the Code

This project is perfect for anyone who wants a deeper understanding of:

- How **self-attention** mixes information  
- Why **residual connections** stabilize deep networks  
- How **feed-forward layers** expand representational capacity  
- What causes **overfitting** in language models  
- How **autoregressive generation** works

Itâ€™s educational first, performant second. :contentReference[oaicite:4]{index=4}

---

## ğŸ§­ Next Steps

If youâ€™re curious where to go after this:

- Add **weight-tying** for embeddings + output head  
- Implement a **KV cache** for faster generation  
- Try **flash-attention-style** optimizations  
- Train with larger corpora or subword tokenization  
- Explore **parameter-efficient fine-tuning** (LoRA)

Each of these opens a new dimension in learning how real LLMs are built and scaled.

---

## ğŸ™ Thanks

Huge thanks to Andrej Karpathy for his clear and joyful teaching â€” his video and code gave me the blueprint for this project. If you want a deep, principled foundation in transformers, start with his walkthrough. :contentReference[oaicite:5]{index=5}

---

If you end up building something cool from this, Iâ€™d love to see it! ğŸš€